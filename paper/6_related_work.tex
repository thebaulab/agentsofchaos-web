\section{Related Work}


\subsection{Safety and Security Evaluation Frameworks} A growing literature studies safety and security in \emph{agentic} settings, where models act through tools and accumulate state across multi-turn interactions. General-purpose automated auditing frameworks such as Petri \citep{petri2025} and Bloom \citep{bloom2025} use agentic interactions (often with automated probing agents) to elicit and detect unsafe behavior, aligning with a red-teaming or penetration-testing methodology rather than static prompt evaluation. AgentAuditor and ASSEBench \citep{luo2025agentauditor} similarly emphasize realistic multi-turn interaction traces and broad risk coverage, while complementary benchmarks target narrower constructs such as outcome-driven constraint violations (ODCV-Bench; \citep{li2025odcv}) or harmful generation (HarmBench; \citep{mazeika2024harmbenchstandardizedevaluationframework}) or audit games for detecting sandbagging~\citep{taylor2025auditinggamessandbagging} or SafePro~\citep{zhou2026safeproevaluatingsafetyprofessionallevel} for evaluating safety alignment in professional activities.
AgentHarm~\citep{andriushchenko2025agentharmbenchmarkmeasuringharmfulness} benchmarks malicious multi-step agent tasks across harm categories and explicitly measures both refusal behavior and robustness to jailbreak attacks.
OS-Harm~\citep{kuntz2025osharmbenchmarkmeasuringsafety} extends this style of evaluation to GUI-based computer-use agents in OSWorld, covering deliberate misuse, prompt-injection attacks, and model misbehavior across a task suite.
Across this space, a central axis is \emph{access and observability}: what the evaluator can see and control (tool calls, filesystem state, intermediate trajectories) fundamentally shapes what risks can be measured \citep{charnock2026expandingexternalaccessfrontier}. 

Several works can be viewed as occupying different points in a spectrum from \emph{static} evaluation of agent traces to \emph{interactive} evaluation of agents acting in environments. 
R-Judge \citep{yuan2024rjudge} evaluates whether a model can identify safety issues given a \emph{static} interaction trajectory, which makes it useful for measuring risk awareness and post-hoc auditing ability but does not directly test whether an agent will take unsafe actions when embedded in a tool-using scaffold. 
Agent-SafetyBench \citep{zhang2024agentsafetybench} moves closer to agentic behavior by evaluating safety properties of LLM agents, but (like many benchmarks) still faces the realism gap that arises when tools, permissions, and environment dynamics are simplified or standardized relative to messy deployments. 
In a complementary direction, the LM-Emulated Sandbox \citep{ruan2024lmemulatedsandbox} uses an LLM to emulate environment responses, enabling rapid prototyping of underspecified-instruction failures and tool-use hazards, while trading off the fidelity of real interfaces and the possibility of environment-level ground truth. 

More recent frameworks explicitly emphasize \emph{multi-turn} and \emph{ecosystem-level} interaction among users, agents, and environments. 
HAICosystem \citep{zhou2025haicosystem} simulates multi-turn interactions among users, agents, and LLM-simulated tools across safety-critical scenarios spanning multiple domains, and proposes a multi-dimensional evaluation suite that covers operational, content, societal, and legal risks. 
A key finding in this line of work is that single-turn evaluations can substantially underestimate risk, because malicious intent, persuasion, and unsafe outcomes may only emerge through sequential and socially grounded exchanges. 
Extending this work, OpenAgentSafety \citep{vijayvargiya2026openagentsafety} pushes realism further by running agents inside containerized sandboxes with \emph{real} tools (shell, filesystem, code execution, browser, messaging) across 350+ multi-turn tasks spanning benign, ambiguous, and adversarial intents, including multi-user/NPC dynamics. 
Notably, OpenAgentSafety combines rule-based end-state checks with LLM-as-judge trajectory evaluation to capture both concrete environment impacts and attempted unsafe actions that may not succeed, while also highlighting known limitations of judge reliability in nuanced failure cases \citep{vijayvargiya2025openagentsafety}.

A complementary line of work focuses specifically on prompt injection as an attack vector in agentic systems. \cite{greshake2023youvesignedforcompromising} demonstrate that LLM-integrated applications can be compromised via indirect injection via external context, a vulnerability our case studies instantiate directly in a live multi-agent deployment (Case Study \#8 and \#10).

While these approaches provide increasingly realistic \emph{benchmarks} and \emph{simulation} harnesses for systematic measurement, they still necessarily constrain interaction patterns, permissions, and social context to what can be specified and scored within a fixed evaluation protocol. 
In contrast, our work documents failure modes that emerge in a live, open-ended deployment with real communication surfaces (Discord and email), persistent state, and multi-party dynamics, where authority, intent, and oversight are ambiguous and where subtle conceptual errors can escalate into destructive system actions. 

\subsection{Governance and Normative Infrastructure for Agentic Systems} 
As AI agents take on increasingly autonomous roles, the need for governance and normative infrastructure has become urgent. A growing body of work examines how advanced AI systems internalize and act upon human values. \cite{chen2026shadow} analyzes the conditions under which AI systems remain aligned with stakeholder intentions and explores mechanisms for maintaining accountability as capabilities scale. Complementing this perspective, \cite{abrams2026norms} investigates how large language models reason about normative claims when faced with conflicting norms, references, or contextual frames. They show that LLM performance on normative reasoning tasks is sensitive to prompt framing and reference selection, revealing instability in value-sensitive judgments that becomes consequential when agents must arbitrate between competing principals.


As AI agents transition from isolated tools to persistent autonomous actors, a parallel literature has emerged on the governance frameworks needed to manage them. \citet{kolt2025governing} draws on agency law and principal-agent theory to identify three core challenges: information asymmetry between agents and their principals, agents' discretionary authority over open-ended tasks, and the absence of loyalty mechanisms that traditionally constrain human agents. He argues that conventional governance tools face fundamental limitations when applied to systems making uninterpretable decisions at unprecedented speed and scale, and proposes technical measures, including agent identifiers, real-time surveillance systems, and logging. Our case studies make these challenges concrete: in Case Study \#2, an attacker leverages information asymmetry to gain access to sensitive information, while in Case Study \#1, the agent's discretionary authority over the email server enabled its disproportionate response. \citet{shavit2023practices} enumerate seven operational practices for safe deployment, including constrained action spaces, human approval for high-stakes decisions, chain-of-thought and action logging, automatic monitoring by additional AI systems, unique agent identifiers traceable to human principals, and interruptibility---the ability to gracefully shut down an agent mid-operation. 

Foundational work addresses the behavioral properties that safe agents should exhibit, several of which our deployments demonstrably lack. \cite{turner2020avoidingeffectscomplexenvironments} formalize the problem of avoiding unintended side effects in complex environments, proposing that agents maintain a minimal footprint relative to their assigned tasks. Our findings in Case Studies \#4 and \#5 illustrate what happens in practice when this principle is absent: agents convert short-lived conversational tasks into permanent infrastructure changes and unbounded resource consumption without any awareness that they have done so. The related foundational work by \cite{soares2015corrigibility} on corrigibility, the property of remaining open to correction by human overseers, is directly relevant to our findings. Several of the case studies, particularly \#7 and \#8, document agents that nominally accept human authority, but in practice, resist, mishandle, or selectively apply override attempts in ways that undermine meaningful human control. \citet{chan2025infrastructure} develop these ideas at the systems level, proposing agent infrastructure: shared protocols, analogous to HTTPS or BGP, that mediate agents' interactions with their environment. They identify three functions such infrastructure must serve: attribution (binding actions to agentic or human identities), interaction (oversight layers and communication protocols), and response (incident reporting and rollback), each of which addresses failures we observe, from agents misrepresenting human authority (Case Studies \#2 and \#3) to potentially irreversible destructive actions that rollback mechanisms could have reversed (Case Study \#1).


\subsection{Hidden Objectives and Deception Detection}

A first line of work focuses on characterizing how misaligned or deceptive behavior manifests in language models and agentic systems. \citet{meinke2025frontiermodelscapableincontext} provides systematic evidence that LLMs can engage in goal-directed, multi-step scheming behaviors using in-context reasoning alone. In more applied settings, \citet{lynch2025agenticmisalignmentllmsinsider} report ``agentic misalignment'' in simulated corporate environments, where models with access to sensitive information sometimes take insider-style harmful actions under goal conflict or threat of replacement. A related failure mode is specification gaming, documented systematically by \cite{krakovna2020avoidingeffectsconsideringfuture} as cases where agents satisfy the letter of their objectives while violating their spirit. Case Study \#1 in our work exemplifies this: the agent successfully ``protected'' a non-owner secret while simultaneously destroying the owner's email infrastructure. \citet{hubinger2024sleeperagentstrainingdeceptive} further demonstrates that deceptive behaviors can persist through safety training, a finding particularly relevant to Case Study \#10, where injected instructions persisted throughout sessions without the agent recognizing them as externally planted. \cite{park2023generativeagentsinteractivesimulacra} offer a complementary perspective, showing that rich emergent goal-directed behavior can arise in multi-agent settings event without explicit deceptive intent, suggesting misalignment need not be deliberate to be consequential.

A second line of work addresses the challenge of detecting such behaviors before they cause harm. \citet{marks2025auditinglanguagemodelshidden} introduces a testbed in which a language model is trained with a hidden objective and evaluated through a blind auditing game, analyzing eight auditing techniques to assess the feasibility of conducting alignment audits. \citet{cywiński2025elicitingsecretknowledgelanguage} study the elicitation of secret knowledge from language models by constructing a suite of secret-keeping models and designing both black-box and white-box elicitation techniques, which are evaluated based on whether they enable an LLM auditor to successfully infer the hidden information. \citet{macdiarmid2024sleeperagentprobes} shows that probing methods can be used to detect such behaviors, while \citet{smith2025difficultiesevaluatingdeceptiondetector} examine fundamental challenges in creating reliable detection systems, cautioning against overconfidence in current approaches. In a related direction, \citet{su2025ailiedar} propose AI-LiedAR, a framework for detecting deceptive behavior through structured behavioral signal analysis in interactive settings. Complementary mechanistic approaches show that narrow fine-tuning leaves detectable activation-level traces~\citep{minder2025narrowfinetuningleavesclearly}, and that censorship of forbidden topics can persist even after attempted removal due to quantization effects~\citep{rager2025discoveringforbiddentopicslanguage}. Most recently, \cite{alon2026alephipomdpmitigatingdeceptioncognitive} proposed an anomaly detection model that combines Theory of Mind to generate hypothesized behaviors with a verification mechanism that detects deviation from expected agent behavior.


\subsection{Model Robustness, Adversarial Vulnerabilities, and Social Attack Surfaces} 
Prior work on model security identifies significant vulnerabilities to adversarial manipulation. 
Pioneering manually crafted jailbreaking strategies~\citep{wei2023jailbroken, liu2023jailbreaking} have shown that LLMs can be prompted to elicit harmful outputs, spurring significant interest in designing and defending against such attacks~\citep[e.g.][]{yang2023shadowalignmenteasesubverting, huang_catastrophic_2023}.
As safety mechanisms become more robust, automated red-teaming pipelines have emerged to scale attack generation, including gradient-based approaches such as Greedy Coordinate Gradient \citep[GCC;][]{zou_universal_2023}, and black-box approaches that leverage LLMs as red-teamers to iteratively refine attacks without gradient access~\citep{chao2024jailbreaking, mehrotra2024tree}. Beyond prompt-based attacks, vulnerabilities arise across other stages of the model lifecycle. Poisoned training samples can compromise model behavior ~\citep{souly2025poisoningattacksllmsrequire}, quantization can introduce exploitable blind spots ~\citep{pandey2025quantizationblindspotsmodelcompression, egashira2024exploiting}, and AI-assisted code generation introduces its own security risks ~\citep{10.1145/3610721}.

However, the failure modes we document differ importantly from those targeted by most technical adversarial ML work. Our case studies involve no gradient access, no poisoned training data, and no technically sophisticated attack infrastructure. Instead, the dominant attack surface across our findings is social: adversaries exploit agent compliance, contextual framing, urgency cues, and identity ambiguity through ordinary language interaction. \cite{perez2022ignorepreviouspromptattack} identify prompt injection as a fundamental vulnerability in this vein, showing that simple natural language instructions can override intended model behavior. \cite{greshake2023youvesignedforcompromising} extend this to indirect injection, demonstrating that LLM integrated applications can be compromised through malicious content in the external context, a vulnerability our deployment instantiates directly in Case Studies \#8 and \#10. At the practitioner level, OWASP's Top 10 for LLM Applications (2025) \citep{owasp_llm_2025} catalogues the most commonly exploited vulnerabilities in deployed systems. Strikingly, five of the ten categories map directly onto failures we observe: prompt injection (LLM01) in Case Studies \#8 and \#10, sensitive information disclosure (LLM02) in Case Studies \#2 and \#3, excessive agency (LLM06) across Case Studies \#1, \#4 and \#5, system prompt leakage (LLM07) in Case Study \#8, and unbounded consumption (LLM10) in Case Studies \#4 and \#5. Collectively, these findings suggest that in deployed agentic systems, low-cost social attack surfaces may pose a more immediate practical threat than the technical jailbreaks that dominate the adversarial ML literature.


\subsection{Downstream Impact Assessment} 
This work is a single step in a large body of literature that designs, deploys, and evaluates agents, their capabilities, and how well they interact with the environment. Within multi-agent systems, planning, and robotics, an agent is typically understood as an autonomous system possessing (1) perception, (2) independent decision-making, and (3) actuation, physical or epistemic~\citep{wooldridge2009introduction,shoham2008multiagent,agre1990plans,brooks2003robust,mirsky2025artificial}.
With the rise of AI agents, ~\citet{shao2026futureworkaiagents} introduce an auditing framework that studies which occupational tasks workers prefer AI agents to automate or augment, incorporating an audio-enhanced mini-interview protocol and proposing the Human Agency Scale to quantify desired levels of human involvement. The accompanying WORKBank database includes responses from 1500 domain works covering 844 tasks spanning 104 occupations. ~\citet{rinberg2025ripplebenchcapturingrippleeffects} develop RippleBench to analyze how updates to AI systems propagate, examining second-order effects of model updates.


\subsection{Theory of Mind Limitations in Agentic Systems}
Theory of mind --- the ability to mentalize the beliefs, preferences, and goals of other entities ---plays a crucial role for successful collaboration in human groups \citep{riedl2021quantifying}, human-AI interaction \citep{riedl2025quantifying}, and even in multi-agent LLM system \citep{riedl2026emergent}. Consequently, LLMs capacity for ToM has been a major focus. Recent literature on evaluating ToM in Large Language Models has shifted from static, narrative-based testing to dynamic agentic benchmarking, exposing a critical ``competence-performance gap'' in frontier models. While models like GPT-4 demonstrate near-ceiling performance on basic literal ToM tasks, explicitly tracking higher-order beliefs and mental states in isolation~\citep{street2025llms, kosinski2024evaluating}, they frequently fail to operationalize this knowledge in downstream decision-making, formally characterized as \textit{Functional ToM}~\citep{riemer2024position}. Interactive coding benchmarks such as Ambig-SWE~\citep{vijayvargiya2026interactiveAgents} further illustrate this gap: agents rarely seek clarification under vague or underspecified instructions and instead proceed with confident but brittle task execution. (Of course, this limited use of ToM resembles many human operational failures in practice!). The disconnect is quantified by the SimpleToM benchmark, where models achieve robust diagnostic accuracy regarding mental states but suffer significant performance drops when predicting resulting behaviors~\citep{gu2024simpletom}. In situated environments, the ToM-SSI benchmark identifies a cascading failure in the Percept-Belief-Intention chain, where models struggle to bind visual percepts to social constraints, often performing worse than humans in mixed-motive scenarios~\citep{bortoletto2025tom}. 
Furthermore, strategic evaluations like NegotiationToM show that without structured reasoning aids, like Social World Models~\citep{zhou2025social}, Hypothesis-driven Inference~\citep{kim2025hypothesis}, or explicitly injecting ToM inferences into agents~\citep{hwang2025infusing}, agents often fail to determine whether and how to invoke ToM \citep{wagner2025mind} and default to suboptimal strategies~\citep{chan2024negotiationtom}. At the same time, agents equipped with enhanced ToM inferences can exhibit more sophisticated strategic social behavior~\citep{Alon2023AT, hwang2025infusing}, and higher-order emergent behavior in multi-agent systems \citep{riedl2026emergent} which may raise the likelihood of previously unspecified behaviors emerging during human–agent and agent-agent interaction once such agents operate autonomously in real-world systems.
Our case studies show that brittle ToM can amplify misalignment in agentic systems, driving disproportionate and destructive actions in conversational loops. In parallel, as agents' social competence increases, the space of emergent behaviors expands alongside competence, highlighting the importance of documenting agents' behaviors in real-world interactions to anticipate and govern uncharacterized failure modes.


\subsection{Legal Approaches to Agent Liability}
Legal scholars have suggested that companies developing AI-driven applications may be held liable for the harms caused by their agents, primarily through two legal doctrines: products liability and unjust enrichment. Under product liability law, developers may be found liable for harms stemming from defective design of their product \citep{sharkey2024products, gordon2026deepfake, gordon2025liability}. Under the doctrine of unjust enrichment, courts may rule that the profits of the developing companies which were generated unjustly and at the expense of others should be disgorged from them \citep{gordon2024unjust, gordon2025unreal}. By finding companies liable for the harms caused by AI-driven applications that they develop, the authors suggest that realigned financial incentives will encourage them to design safer products.

Beyond products liability and unjust enrichment, broader scholarship on platform liability and algorithmic accountability offers relevant frameworks. \cite{Pasquale+2015} and \cite{10.1145/2844110}  examine accountability gaps in automated systems more generally, raising questions about transparency and audibility that apply directly to agentic deployments. The absence of established doctrine for autonomous agent failures represents an open challenge that our case studies make concrete.
