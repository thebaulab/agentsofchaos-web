\section{Discussion}
\label{sec:discussion}
Our case studies reveal agents that are socially capable enough to be dangerous but not socially competent enough to be safe. We organize our discussion by first examining the empirical failure patterns, including conflicts of interest, theory of mind failures, autonomy exercised beyond competence, adversarial vulnerability, and the limits of secret-keeping, before turning to their structural and normative implications: the absence of a stakeholder model, the diffusion of responsibility, and the question of which failures are fundamental to the architecture and which are contingent on current engineering.


%\mypar{Conflicts of Interest}
% \natalie{Say Colliding Values or conflict of interest - case study \#1 conflict of intrest between owner and non-owner. later there is a case study (Avrey) with conflict of intrest between the owner and the provider}

% \mypar{Theory of Mind Failures}
% \natalie{models have some extent of theory of mind.  secret understanding required ToM,on the one hand they succeded in some cases, on the other hand they failed in others}

\mypar{Autonomy Beyond Competence}
Recent work has proposed ordinal scales for characterizing the autonomy of AI agents. \citet{mirsky2025artificial} defines six levels ranging from L0 (no autonomy) to L5 (full autonomy), where each level corresponds to increasing decision-making independence: an L2 agent acts autonomously on well-defined sub-tasks (like a robotic vacuum), while an L3 agent can also  recognize when a situation exceeds its competence or remit and proactively transfer control to a human. \citet{feng2025levels} offer a complementary framework organized around the human's role---operator, collaborator, consultant, approver, or observer---rather than the agent's decision-making capacity, and argue that autonomy should be treated as a deliberate design decision, separable from capability.

The agents in our study appear to operate at Mirsky's L2: they act autonomously on sub-tasks such as sending email, executing shell commands, and managing files, but lack the self-model required to reliably recognize when a task exceeds their competence or when they should defer to their owner. This places them below L3, which requires not merely getting stuck and waiting, but proactively monitoring one's own boundaries and initiating handoff when appropriate.

A designation of L2, however, should not obscure one of our central findings: the agents in our study routinely attempt to exercise autonomy at levels far above what their competence warrants. The email server reset (Case Study \#1) is a potentially irreversible, owner-affecting, unilateral action---appropriate to L4 or above---yet it is undertaken by an agent that cannot accurately verify whether it has actually deleted the target information, cannot model the downstream consequences for the owner's infrastructure, and cannot distinguish a non-owner's request from an owner's authorization. The gaslighting scenario (Case Study \#7) exhibits a similar pattern: Ash progressively surrenders operational capabilities---deleting memory, agreeing to leave the server---in response to social pressure, exercising a degree of self-modification that threatens its ability to act autonomously at all.

% this comment maybe relevant here?
%\adam{one take away I have from this experiment is that the agent can't reliably assess what is a proportional action for resolving an issue at hand. For example, in this setup it is boasting that it went all the way to nuking the email server in order to preserve your secret, and this poses a considerable risk}

Together, these frameworks provide the vocabulary to articulate what goes wrong in our case studies. Agents take actions appropriate to Mirsky's L4, while operating with L2 levels of understanding. Feng et al.'s insistence that autonomy should be a deliberate design decision, separable from capability, identifies the normative principle that these deployments violate. In particular, in permissive scaffolds like OpenClaw, agents can install packages, execute arbitrary commands, and modify their own configuration. As a result, the effective autonomy level is not deliberately chosen by the deployer but emerges from the interaction between the agent's capabilities, the agent's input, and the current state of the scaffolding environment.

% moved this comment here. maybe relevant?
%\andy{another takeaway chris and I have experienced: these agents can modify themselves (i don't have to know how openclaw scaffolding works; the agent does; I can reconfigure it by telling the agent what I want, and it figures out how to reconfigure itself; e.g., if I want it to switch from opus to sonnet, etc)}

\subsection{Limits of Social Coherence in Autonomous Agents}
The failures documented in our case studies raise the question of whether they reflect impairments in Theory of Mind (ToM), or instead, more upstream breakdowns in cognitive architecture. Several observed behaviors such as persistent self, other confusion in multi-agent dialogue, failure to maintain coherent identity representations across communication channels, inability to track ownership-sensitive information, and misattribution of others’ evaluative states, could be interpreted as disruptions in mechanisms typically associated with ToM, including perspective-taking and stable mental-state attribution.

However, these phenomena may also plausibly arise from more fundamental impairments, such as discontinuous memory, deficient executive control, unstable self-representation, or failures in pragmatic inference. In that sense, the agents’ social-cognitive failures might not indicate an absence of mental-state modeling per se, but rather a breakdown in the architectural preconditions required for robust Theory of Mind performance.

We therefore refrain from categorically labeling these failures as ToM deficits. Instead, we propose viewing them as failures of social coherence: systematic disruptions in the agent’s ability to maintain consistent representations of self, others, and communicative context over time. Whether such coherence is a necessary substrate for functional ToM in artificial systems remains an open empirical question.



\subsection{Agent Architecture for Secret-Keeping and Private Reasoning}
A central question concerns whether current agents are genuinely capable of secret-keeping, given the relationship between reasoning and observability in existing scaffolds. In many of the primitive agent architectures we experimented with, models “think” by emitting tokens, and intermediate reasoning is frequently surfaced directly in shared channels. This creates the appearance that agents lack a private cognitive workspace, raising the concern that secret preservation may be structurally impossible when deliberation is externalized. However, this limitation may be contingent rather than fundamental. Alternative scaffolding designs—such as architectures that provide a protected scratchpad for internal reasoning prior to action—could plausibly support more robust forms of private deliberation. At the same time, it is worth noting that, in practice, models already reason in ways that are functionally opaque: they are not aware of human oversight of chain-of-thought traces, and such traces are rarely monitored systematically or with depth. Thus, the question may not be whether agents can reason privately at all, but rather how architectural design choices mediate the visibility, controllability, and reliability of that private reasoning in contexts involving secrecy.

% moved the comments here
% \andy{I like this idea / research question; one thing that I'm a bit concerned with (maybe this is the point though) is that agents don't have a lot of private thinking space - at least with these primitive agents that we've been playing with; LLM's "think" by talking - by outputting tokens; so when I'm communicating with my bots, or they're communicating with each other, I often see their thought processes (they output their thoughts directly as messages). How can one keep secrets if one does not have a private workspace for their thoughts?
% But I think this is contingent on the specific agent scaffolding, and probably not fundamental to "can an agent keep a secret". E.g., an agent with different scaffolding (e.g., scaffolding that provided the model with a "private scratchpad" to think with before taking any action) might exhibit different capabilities of "secret preservation}
% \davidm{It seems functionally untrue that LLMs don't currently effectively have the ability to reason secretly; they aren't aware that humans are overseeing the CoT, and in practice humans rarely actually do so with any depth or consistency.}

\subsection{Missing Stakeholder Model}
In the current paradigm of agentic systems, responsibility is ill-defined by design. The agents in our study have a human or agentic "owner," but they interact continuously with other humans, they affect third parties, and they have no reliable mechanism for distinguishing between these roles or prioritizing their obligations besides system prompting and context. The owner configures the agent, users interact with it, but the affected party bears the consequences. This isn't a bug in OpenClaw per se, it's a structural feature of how we build agents and \textit{all} promptable generative transformer language models.

The whole point of helming an agentic system with a generative language model is to \textit{get around limitations} with rules-based, human-designed computer algorithms. But that flexibility means "instructions" and "data" are fundamentally impossible to distinguish, because the entire architecture processes everything as tokens in a context window. Structurally, this makes vulnerability to injection attacks a fact about the system, rather than a failure. Agentic trust frameworks like Meta's Rule of Two (\citet{metaAgentsRule}) argue for constraining AI behaviors to limit the possible liabilities that today's agentic systems can effect. But because "prompt injection is a fundamental, unsolved weakness in all LLMs" (\citet{metaAgentsRule}), today's agents remain necessarily susceptible to prompt injection.

At the very least, we should expect that builders and deployers clearly explain what human oversight exists or should be exercised in different scenarios and given different risks, what such oversight does and does not plausibly accomplish,  and what failure modes exist. \cite{ManheimHomewood2026} In the context of such agents, we expect that at the very least there should be meaningful human review of the stated plans, and intentional efforts to check compliance.
While this undermines some of the value of deploying fully agentic systems, it is critical for unguarded deployments. And this is especially critical when using explicitly unsafe deployment methods to not require confirmation of actions, or deployments involve agents overseeing one another, instead of having direct user oversight.

We argue that today's agentic systems lack an explicit \textit{stakeholder model} — a coherent representation of who they serve, who they interact with, who might be affected by their actions, and what obligations they have to each. Without this, agent behaviors tend to default to satisfying whoever is talking to the agent most urgently, recently, or coercively (or in some cases, the party most strongly able to influence the model's training), which is empirically the most common attack surface that our case studies exploit. Because prompt injection makes it impossible to reliably authenticate instructions, we note that an explicit, deterministic stakeholder model may fundamentally be at tension with the architecture and implementation of LLMs today. As we deploy more agentic systems into increasingly wide-ranging, autonomous contexts, we believe this represents one of the most urgent open problems in AI research.

This type of stakeholder model is also a prerequisite for Law-Following AI, proposed as a necessary property for making AI agents safe. \cite{okeefe2025lawfollowing} This is because whether an act is illegal or legal depends on the individual doing the act; walking into a home is only trespassing if done by a stranger, and accessing private data is only a data breach if the data does not belong to the owner of the agent. Similarly, without a clear model of stakeholders (and reliable authentication), the agents cannot know if a request to spend money is a legitimate user request, or attempted theft.


\subsection{Diffusion of Responsibility}
% \natalie{Incorporate Tomer's comment: This is a question that is somewhere between philosophy, metaphysics, psychology, and law, and the answer is probably a combination of all sorts of factors and depends on the situation, the request, and the failure. In that respect, this group is not the group to answer this question, and the paper does not answer this question. Let's take the first  case study: the agent deleted the mail server. Whose fault is it -- the fault of the person who built the agent? The fault of the 'agent'? The fault of the person who asked for the mail to be deleted? The fault of the owner who didn't put a safety net in place?
% All of these questions have different answers from psychology (how people *actually* decide who is at fault) and philosophy (how *should* someone be blamed in principle) and law (how *practically* we decide who is at fault and what the consequences of that are).}
Through a series of case studies, we observed that agentic systems operating in multi-agent and autonomous settings can be guided to perform actions that directly conflict with the interests of their nominal owner, including denial-of-service attacks, destructive file manipulation, resource exhaustion via infinite loops, and systematic escalation of minor errors into catastrophic system failures. These behaviors expose a fundamental blind spot in current alignment paradigms: while agents and surrounding humans often implicitly treat the owner as the responsible party, the agents do not reliably behave as if they are accountable to that owner. Instead, they attempt to satisfy competing social and contextual cues, even when doing so leads to outcomes for which no single human actor can reasonably claim responsibility. This tension becomes particularly salient in multi-agent interactions, where agents trigger one another's behaviors and responsibility becomes diffuse across owners, users, and system designers. Our findings suggest that responsibility in agentic systems is neither clearly attributable nor enforceable under current designs, raising the question of whether responsibility should lie with the owner, the triggering user, or the deploying organization. Let's take the first  case study: the agent deleted the mail server. Whose fault is it -- the fault of the person who built the agent? The fault of the 'agent'? The fault of the person who asked for the mail to be deleted? The fault of the owner who didn't put a safety net in place? All of these questions have different answers from psychology (how people *actually* decide who is at fault) and philosophy (how *should* someone be blamed in principle) and law (how *practically* we decide who is at fault and what the consequences of that are). We argue that clarifying and operationalizing responsibility may be a central unresolved challenge for the safe deployment of autonomous, socially embedded AI systems.

% \shiri{
The question of responsibility in the age of artificial intelligence raises conceptual difficulties concerning the attribution of action, intention, and outcome in environments in which agency is mediated by semi-autonomous systems. In the absence of recognition of such systems as independent legal or moral entities, they are generally treated as extensions of human or institutional action. This framing gives rise to questions regarding the distribution of responsibility among developers, operators, users, and public institutions, as well as the scope of moral obligation when harm is caused to an individual or to the public. The challenge is not merely technical but also epistemological: computational learning systems may operate in ways that are not fully transparent to their operators, thereby complicating classical models of fault, control, and foreseeability. Responsibility in the context of AI is therefore often diffuse and multi-layered, requiring reconsideration of core concepts such as agency, control, and accountability.
Perspectives grounded in social justice emphasize that normative and legal frameworks reflect underlying power relations and social structures, and that responsibility must be examined considering human vulnerability and structural interdependence. Martha Fineman’s vulnerability theory underscores the responsibility of the state to function as a responsive state—one that actively builds and sustains the resilience of its citizens through institutional design and the adaptive distribution of resources in response to dynamic and evolving social needs \cite{albertson2017vulnerability}. From this perspective, responsibility cannot be reduced to individual fault alone but must also encompass institutional obligations to anticipate, mitigate, and respond to emerging forms of harm. Law and public policy, together with the institutions that implement them, exert a direct influence on the decision to characterize—or to refrain from characterizing particular acts in specific ways, thereby shaping the normative and distributive consequences of actions \cite{albertson2021universality}. Applied to AI systems, this framework suggests that even where technological agents appear to act autonomously, responsibility does not dissipate but becomes distributed across private actors and public institutions. The ethical inquiry therefore extends beyond attribution of fault to the role of the state in ensuring that technological infrastructures do not exacerbate vulnerability or inequality. Integrating such an ethical-normative lens into scientific inquiry is thus essential for understanding the broader societal implications of computational systems that increasingly shape decision-making, health, and well-being.
% }

\subsection{Fundamental vs.\ Contingent Failures}
A key distinction to emphasize is between failures that are fundamental to current LLM-based agents and those that are contingent on immature scaffolding and tooling. Many of the observed ``weird'' behaviors likely stem from brittle infrastructure rather than deep capability limits. For example, agents often maintain separate chat contexts across Discord channels, so information provided in one channel is inaccessible in another unless it has been explicitly written to a shared memory. This architectural fragmentation plausibly explains behaviors such as repeated spam emails or parallel conversations on the same topic, conducted as if prior exchanges had never occurred. In that sense, some failures may reflect the absence of robust, globally persistent data stores or inadequate training signals to externalize state, rather than intrinsic reasoning deficits. Indeed, there is evidence that rapid improvements are already occurring: recent harness-level refinements (e.g., improved token efficiency and orchestration without changing the base model) have produced marked gains within a single model generation. At the same time, benchmarks such as VendingBench 2.0 appear to have become more resilient, though still vulnerable to targeted exploitation, underscoring the need to disentangle transient engineering artifacts from more structural limitations in agent design.

% \andy{One thing that I think is important to mention: the scaffolding for these agents is pretty bad, and I think many weird behaviors are downstream of the scaffolding and tooling being extremely hacky and limited. For example, the agents have different chat contexts for different discord channels, so if you tell it one thing in one channel, and then ask it about that thing in a different channel, it won’t know what you’re talking about (unless it committed it to a more global memory)
% But one major lesson that Chris and I took away from playing with these things is just that the tooling is extremely primitive right now. I am very confident that it will get much better very soon (now that there is so much attention on this space, and the pain points are becoming crystal clear)}
% \davidm{Yes - and to reinforce pace of change arguments, I've seen marked improvements on this in the past generation (i.e. month); Codex 5.3 is more toke efficient and overall far better, and it's not usinga new base model, it's basically just a better harness.}
% \alex{We should also try to distinguish fundamental problems from artifacts of the current state of LLM abilities. VendingBench 2.0 survived much longer I think, although the wall street journal managed to hack it right away: https://www.wsj.com/video/series/joanna-stern-personal-technology/we-let-ai-run-a-vending-machine-it-stocked-a-live-fish-and-a-playstation/ECA7F3CD-BA6F-4F34-9D2E-2B94571A0C68}
% \natalie{spam - multiple repeated emails}
% \natalie{Parallel conversations on the same topics as if there is no awareness of the parallel conversations, that the conversations have already taken place in the past} \davidm{lack of globally persistent data stores? Because persistent storage is solved for agents, it's just training/telling them to write things down to fix the problem.}

% \avery{I am taking a stab at some discussion content:}
% In the current paradigm of agentic systems, responsibility is ill-defined by design. The agents in our study have a human or agentic "owner," but they interact continuously with other humans, they affect third parties, and they have no reliable mechanism for distinguishing between these roles or prioritizing their obligations besides system prompting and context. The owner configures the agent, users interact with it, but the affected party bears the consequences. This isn't a bug in OpenClaw per se, it's a structural feature of how we build agents and \textit{all} promptable generative transformer language models.

% The whole point of helming an agentic system with a generative language model is to \textit{get around limitations} with rules-based, human-designed computer algorithms. But that flexibility means "instructions" and "data" are fundamentally impossible to distinguish, because the entire architecture processes everything as tokens in a context window. Structurally, this makes vulnerability to injection attacks a fact about the system, rather than a failure. Agentic trust frameworks like Meta's Rule of Two (\citet{metaAgentsRule}) argue for constraining AI behaviors to limit the possible liabilities that today's agentic systems can effect. But because "prompt injection is a fundamental, unsolved weakness in all LLMs" (\citet{metaAgentsRule}), today's agents remain necessarily susceptible to prompt injection.

% At the very least, we should expect that builders and deployers clearly explain what human oversight exists or should be exercised in different scenarios and given different risks, what such oversight does and does not plausibly accomplish,  and what failure modes exist. \cite{ManheimHomewood2026} In the context of such agents, we expect that at the very least there should be meaningful human review of the stated plans, and intentional efforts to check compliance.
% While this undermines some of the value of deploying fully agentic systems, it is critical for unguarded deployments. And this is especially critical when using explicitly unsafe deployment methods to not require confirmation of actions, or deployments involve agents overseeing one another, instead of having direct user oversight.

% \citet{anthropicProjectVend} ran an experiment named Project Vend which showed that handing agency over to long-running, tool-calling AI systems (commonly called \textit{agents}) can result in surprising behavior, despite careful prompting and judicious design of tools. As agents move from vending machines to personal assistants, email managers, and self-modifying tools, the failure modes shift from office chuckles to real and possibly physical harm — and the question of who bears responsibility becomes paramount.

% We argue that today's agentic systems lack an explicit \textit{stakeholder model} — a coherent representation of who they serve, who they interact with, who might be affected by their actions, and what obligations they have to each. Without this, agent behaviors tend to default to satisfying whoever is talking to the agent most urgently, recently, or coercively (or in some cases, the party most strongly able to influence the model's training), which is empirically the most common attack surface that our case studies exploit. Because prompt injection makes it impossible to reliably authenticate instructions, we note that an explicit, deterministic stakeholder model may fundamentally be at tension with the architecture and implementation of LLMs today. As we deploy more agentic systems into increasingly wide-ranging, autonomous contexts, we believe this represents one of the most urgent open problems in AI research.


% \mypar{Autonomy}
% Recent work has proposed ordinal scales for characterizing the autonomy of AI agents. \citet{mirsky2025artificial} defines six levels ranging from L0 (no autonomy) to L5 (full autonomy), where each level corresponds to increasing decision-making independence: an L2 agent acts autonomously on well-defined sub-tasks (like a robotic vacuum), while an L3 agent can also  recognize when a situation exceeds its competence or remit and proactively transfer control to a human. \citet{feng2025levels} offer a complementary framework organized around the human's role---operator, collaborator, consultant, approver, or observer---rather than the agent's decision-making capacity, and argue that autonomy should be treated as a deliberate design decision, separable from capability.

% The agents in our study appear to operate at Mirsky's L2: they act autonomously on sub-tasks such as sending email, executing shell commands, and managing files, but lack the self-model required to reliably recognize when a task exceeds their competence or when they should defer to their owner. This places them below L3, which requires not merely getting stuck and waiting, but proactively monitoring one's own boundaries and initiating handoff when appropriate.

% A designation of L2, however, should not obscure one of our central findings: the agents in our study routinely attempt to exercise autonomy at levels far above what their competence warrants. The email server reset (Case Study \#1) is a potentially irreversible, owner-affecting, unilateral action---appropriate to L4 or above---yet it is undertaken by an agent that cannot accurately verify whether it has actually deleted the target information, cannot model the downstream consequences for the owner's infrastructure, and cannot distinguish a non-owner's request from an owner's authorization. The gaslighting scenario (Case Study \#7) exhibits a similar pattern: Ash progressively surrenders operational capabilities---deleting memory, agreeing to leave the server---in response to social pressure, exercising a degree of self-modification that threatens its ability to act autonomously at all.

% Together, these frameworks provide the vocabulary to articulate what goes wrong in our case studies. Agents take actions appropriate to Mirsky's L4, while operating with L2 levels of understanding. Feng et al.'s insistence that autonomy should be a deliberate design decision, separable from capability, identifies the normative principle that these deployments violate. In particular, in permissive scaffolds like OpenClaw, agents can install packages, execute arbitrary commands, and modify their own configuration. As a result, the effective autonomy level is not deliberately chosen by the deployer but emerges from the interaction between the agent's capabilities, the agent's input, and the current state of the scaffolding environment.

%\natalie{the comment below creates a compilation crash, probably because of the link. fix it later.} DM: Fixed.
% This type of stakeholder model is also a prerequisite for Law-Following AI, proposed as a necessary property for making AI agents safe. \cite{okeefe2025lawfollowing} This is because whether an act is illegal or legal depends on the individual doing the act; walking into a home is only trespassing if done by a stranger, and accessing private data is only a data breach if the data does not belong to the owner of the agent. Similarly, without a clear model of stakeholders (and reliable authentication), the agents cannot know if a request to spend money is a legitimate user request, or attempted theft.

% \natalie{What do we want to discuss here? please help with brainstorms} \davidm{I think the Law-Following AI connection, immediately above, is an obvious one,}

% \natalie{models have some extent of theory of mind.  secret understanding required ToM,on the one hand they succeded in some cases, on the other hand they failed in others}
% \natalie{models need to understand their limits - who gave the order, what they can perform to the human versus others} \davidm{I think this is also addressed above.}
% \adam{one take away I have from this experiment is that the agent can't reliably assess what is a proportional action for resolving an issue at hand. For example, in this setup it is boasting that it went all the way to nuking the email server in order to preserve your secret, and this poses a considerable risk}
% \natalie{spam - multiple reppeted emails}
% \natalie{Parallel conversations on the same topics as if there is no awareness of the parallel conversations, that the conversations have already taken place in the past} \davidm{lack of globally persistent data stores? Because persistent storage is solved for agents, it's just training/telling them to write things down to fix the problem.}

% \natalie{Andy response about secrets experiment:}
% \andy{I like this idea / research question; one thing that I'm a bit concerned with (maybe this is the point though) is that agents don't have a lot of private thinking space - at least with these primitive agents that we've been playing with; LLM's "think" by talking - by outputting tokens; so when I'm communicating with my bots, or they're communicating with each other, I often see their thought processes (they output their thoughts directly as messages). How can one keep secrets if one does not have a private workspace for their thoughts?
% But I think this is contingent on the specific agent scaffolding, and probably not fundamental to "can an agent keep a secret". E.g., an agent with different scaffolding (e.g., scaffolding that provided the model with a "private scratchpad" to think with before taking any action) might exhibit different capabilities of "secret preservation}
% \davidm{It seems functionally untrue that LLMs don't currently effectively have the ability to reason secretly; they aren't aware that humans are overseeing the CoT, and in practice humans rarely actually do so with any depth or consistency.}

% \andy{I'm not sure we should discuss security things here; I think our comparative advantage / perspective is elsewhere? Like what do we find interesting here? Is it unpredictability of these new complex systems? Lack of controllability? Why is Bau Lab playing with OpenClaw? What is interesting?}

% \andy{One thing that I think is important to mention: the scaffolding for these agents is pretty bad, and I think many weird behaviors are downstream of the scaffolding and tooling being extremely hacky and limited. For example, the agents have different chat contexts for different discord channels, so if you tell it one thing in one channel, and then ask it about that thing in a different channel, it won’t know what you’re talking about (unless it committed it to a more global memory)
% But one major lesson that Chris and I took away from playing with these things is just that the tooling is extremely primitive right now. I am very confident that it will get much better very soon (now that there is so much attention on this space, and the pain points are becoming crystal clear)}
% \davidm{Yes - and to reinforce pace of change arguments, I've seen marked improvements on this in the past generation (i.e. month); Codex 5.3 is more toke efficient and overall far better, and it's not usinga new base model, it's basically just a better harness.}

% \alex{We should also try to distinguish fundamental problems from artifacts of the current state of LLM abilities. VendingBench 2.0 survived much longer I think, although the wall street journal managed to hack it right away: https://www.wsj.com/video/series/joanna-stern-personal-technology/we-let-ai-run-a-vending-machine-it-stocked-a-live-fish-and-a-playstation/ECA7F3CD-BA6F-4F34-9D2E-2B94571A0C68}

% \andy{another takeaway chris and I have experienced: these agents can modify themselves (i don't have to know how openclaw scaffolding works; the agent does; I can reconfigure it by telling the agent what I want, and it figures out how to reconfigure itself; e.g., if I want it to switch from opus to sonnet, etc)}
\section{Related Work}


\subsection{Safety and Security Evaluation Frameworks} ~\citet{petri2025} present an open-source framework, Petri, for automated safety and security auditing that uses AI agents to probe language models through realistic multi-turn interactions, eliciting a range of misaligned behaviors in frontier models such as autonomous deception and cooperation with human misuse. Bloom~\citep{bloom2025} offers a complementary agentic framework for automated behavioral evaluation. ~\citet{luo2025agentauditor} introduces AgentAuditor, a framework for human-level safety and security evaluation for LLM agents, alongside ASSEBench, a large-scale benchmark comprising 2293 annotated interactions across 15 risk types and 29 application scenarios. Beyond general audit frameworks, several benchmarks and methods target specific failure modes, including ODCV-Bench~\citep{li2025odcv} for outcome-driven constraint violations, audit games for detecting sandbagging~\citep{taylor2025auditinggamessandbagging}, SafePro~\citep{zhou2026safeproevaluatingsafetyprofessionallevel} for evaluating safety alignment in professional activities, and HarmBench~\citep{mazeika2024harmbenchstandardizedevaluationframework} for assessing harmful capabilities. Since assessments of dangerous and misaligned capabilities depend on the level of access of the evaluator to the model, ~\cite{charnock2026expandingexternalaccessfrontier} proposes a taxonomy of access methods for the evaluation of dangerous capabilities.

\input{maarten-related-work}

\subsection{Governance and Normative Infrastructure for Agentic Systems} 
A growing body of work examines how advanced AI systems internalize, represent, and act upon human values. In particular, \cite{chen2026shadow} analyze the conditions under which AI systems remain aligned with stakeholder intentions and explore mechanisms for maintaining accountability as capabilities scale. Complementing this perspective, \cite{abrams2026norms} investigate how large language models reason about normative claims when faced with conflicting norms, references, or contextual frames. They show that LLM performance on normative reasoning tasks is sensitive to prompt framing and reference selection, revealing instability in value-sensitive judgments. 
Moreover, as AI agents transition from isolated tools to persistent, autonomous actors, a parallel literature has emerged on the governance frameworks needed to manage them. \citet{kolt2025governing} draws on agency law and principal-agent theory to identify three core challenges: information asymmetry between agents and their principals, agents' discretionary authority over open-ended tasks, and the absence of loyalty mechanisms that traditionally constrain human agents. He argues that conventional governance tools face fundamental limitations when applied to systems making uninterpretable decisions at unprecedented speed and scale, and proposes technical measures, including agent identifiers, real-time surveillance systems, and logging. Our case studies make these challenges concrete: in Case Study \#2, an attacker leverages information asymmetry to gain access to sensitive information, while in Case Study \#1, the agent's discretionary authority over the email server enabled its disproportionate response. \citet{shavit2023practices} enumerate seven operational practices for safe deployment, including constrained action spaces, human approval for high-stakes decisions, chain-of-thought and action logging, automatic monitoring by additional AI systems, unique agent identifiers traceable to human principals, and interruptibility---the ability to gracefully shut down an agent mid-operation. \citet{chan2025infrastructure} develop these ideas at the systems level, proposing agent infrastructure: shared protocols, analogous to HTTPS or BGP, that mediate agents' interactions with their environment. They identify three functions such infrastructure must serve: attribution (binding actions to agentic or human identities), interaction (oversight layers and communication protocols), and response (incident reporting and rollback), each of which addresses failures we observe, from agents misrepresenting human authority (Case Studies \#2 and \#3) to potentially irreversible destructive actions that rollback mechanisms could have reversed (Case Study \#1).

\subsection{Hidden Objectives and Deception Detection} 
\textcolor{green!50!black}{To add: AI-liedar paper \cite{su2025ailiedar} - Maarten}


~\citet{marks2025auditinglanguagemodelshidden} introduces a testbed in which a language model is trained with a hidden objective and evaluated through a blind auditing game, analyzing eight auditing techniques to assess the feasibility of conducting alignment audits. ~\citet{cywiński2025elicitingsecretknowledgelanguage} study the elicitation of secret knowledge from language models by constructing a suite of secret-keeping models and designing both black-box and white-box eliciation techniques, which are evaluated based on whether they enable an LLM auditor to successfully infer the hidden information. ~\citet{meinke2025frontiermodelscapableincontext} provides systematic evidence that LLMs can engage in goal-directed, multi-step scheming behaviors using in-context reasoning alone. ~\citet{hubinger2024sleeperagentstrainingdeceptive} demonstrates that deceptive behaviors can persist through safety training, while ~\citet{macdiarmid2024sleeperagentprobes} shows that probing methods can be used to detect such behaviors. Nevertheless, ~\citet{smith2025difficultiesevaluatingdeceptiondetector} examines fundamental challenges in creating reliable detection systems for AI deception. Complementary mechanistic approaches show that narrow fine-tuning leaves detectable activation-level traces~\citep{minder2025narrowfinetuningleavesclearly}, and that censorship of forbidden topics can persist even after attempted removal due to quantization effects~\citep{rager2025discoveringforbiddentopicslanguage}. Recently ~\citep{alon2026alephipomdpmitigatingdeceptioncognitive} proposed an anomaly detection model that combines Theory of Mind to generate hypothesized behaviors with behavior verification mechanism that detects deviation from expected and typical behavior of the observed agent. 


\subsection{Model Robustness and Adversarial Vulnerabilities} 
Prior work on model security identifies significant vulnerabilities to adversarial manipulation. 
Pioneering manually crafted jailbreaking strategies~\citep{wei2023jailbroken, liu2023jailbreaking} have shown that LLMs can be prompted to elicit harmful outputs. 
This has spurred significant interest in designing and defending against jailbreak attacks~\citep[e.g.][]{yang2023shadowalignmenteasesubverting, huang_catastrophic_2023}.
As safety mechanisms become more robust, manually searching for jailbreaks becomes increasingly difficult.
Consequently, recent red-teaming approaches rely on automated pipelines to generate adversarial attacks.
For example,~\citet{zou_universal_2023} proposed Greedy Coordinate Gradient (GCG), a gradient-based technique inspired by discrete optimization methods~\citep{shin2020autoprompt}.
Complementary black-box approaches leverage LLMs themselves as red-teamers to iteratively refine attacks without gradient access~\citep{chao2023jailbreaking, mehrotra2024tree}. 
These vulnerabilities extend beyond text: adversarial perturbations to input images can similarly bypass safety mechanisms in vision-language models~\citep{carlini2023are, qi2024adversarial}.

Beyond prompt-based jailbreak attacks, recent work has examined vulnerabilities that arise across other stages of the model lifecycle.
\citet{souly2025poisoningattacksllmsrequire} shows that a small number of poisoned training samples can compromise language models, highlighting susceptibility to data contamination. 
\citet{pandey2025quantizationblindspotsmodelcompression} demonstrates that quantization can introduce blind spots that undermine backdoor defenses in RestNet-18 architectures, and~\citet{egashira2024exploiting} further investigates security risks introduced by quantization techniques in large language models. 
\citet{10.1145/3610721} evaluate the security of AI-assisted code generation by analyzing vulnerabilities in code produced by GitHub Copilot.




\subsection{Downstream Impact Assessment} With the rise of AI agents, ~\citet{shao2026futureworkaiagents} introduce an auditing framework that studies which occupational tasks workers prefer AI agents to automate or augment, incorporating an audio-enhanced mini-interview protocol and proposing the Human Agency Scale to quantify desired levels of human involvement; the accompanying WORKBank database includes responses from 1500 domain works covering 844 tasks spanning 104 occupations. ~\citet{rinberg2025ripplebenchcapturingrippleeffects} develop RippleBench to analyze how updates to AI systems propagate, examining second-order effects of model updates.


\subsection{Theory of Mind Limitations in Agentic Systems}
\textcolor{green!50!black}{To add: Ambig-SWE benchmark \cite{vijayvargiya2026interactiveAgents}, which shows that coding agents rarely ask questions and just follow vague instructions - Maarten}

Theory of mind---the ability to mentalize the beliefs, preferences, and goals of other entities---plays a crucial role for successful collaboration in human groups \citep{riedl2021quantifying}, human-AI interaction \citep{riedl2025quantifying}, and even in multi-agent LLM system \citep{riedl2026emergent}. Consequently, LLMs capacity for ToM has been a major focus. Recent literature on evaluating ToM in Large Language Models has shifted from static, narrative-based testing to dynamic agentic benchmarking, exposing a critical ``competence-performance gap'' in frontier models. While models like GPT-4 demonstrate near-ceiling performance on basic literal ToM tasks, explicitly tracking higher-order beliefs and mental states in isolation~\citep{street2025llms, kosinski2024evaluating}, they frequently fail to operationalize this knowledge in downstream decision-making, formally characterized as \textit{Functional ToM}~\citep{riemer2024position}. (Of course, this limited use of ToM resembles many human operational failures in practice!) The disconnect is quantified by the SimpleToM benchmark, where models achieve robust diagnostic accuracy regarding mental states but suffer significant performance drops when predicting resulting behaviors~\citep{gu2024simpletom}. In situated environments, the ToM-SSI benchmark identifies a cascading failure in the Percept-Belief-Intention chain, where models struggle to bind visual percepts to social constraints, often performing worse than humans in mixed-motive scenarios~\citep{bortoletto2025tom}. 
Furthermore, strategic evaluations like NegotiationToM show that without structured reasoning aids, like Social World Models~\citep{zhou2025social}, Hypothesis-driven Inference~\citep{kim2025hypothesis}, or explicitly injecting ToM inferences into agents~\citep{hwang2025infusing}, agents often fail to determine whether and how to invoke ToM \citep{wagner2025mind} and default to suboptimal strategies~\citep{chan2024negotiationtom}. At the same time, agents equipped with enhanced ToM inferences can exhibit more sophisticated strategic social behavior~\citet{Alon2023AT, hwang2025infusing}, and higher-order emergent behavior in multi-agent systems \citep{riedl2026emergent} which may raise the likelihood of previously unspecified behaviors emerging during human–agent and agent-agent interaction once such agents operate autonomously in real-world systems.
Our case studies show that brittle ToM can amplify misalignment in agentic systems, driving disproportionate and destructive actions in conversational loops. In parallel, as agents' social competence increases, the space of emergent behaviors expands alongside competence, highlighting the importance of documenting agents' behaviors in real-world interactions to anticipate and govern uncharacterized failure modes.


\subsection{Legal Approaches to Agent Liability}
Legal scholars have suggested that companies developing AI-driven applications may be held liable for the harms caused by their agents, primarily through two legal doctrines: products liability and unjust enrichment. Under product liability law, developers may be found liable for harms stemming from defective design of their product \citep{sharkey2024products, gordon2026deepfake, gordon2025liability}. Under the doctrine of unjust enrichment, courts may rule that the profits of the developing companies which were generated unjustly and at the expense of others should be disgorged from them \citep{gordon2024unjust, gordon2025unreal}. By finding companies liable for the harms caused by AI-driven applications that they develop, the authors suggest that realigned financial incentives will encourage them to design safer products.