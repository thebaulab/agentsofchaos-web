\section{Discussion}
\label{sec:discussion}

Our case studies reveal agents that are strong enough to perform various complex tasks, but don't always carry them out in a safe manner. We organize this discussion by first characterizing what we observed, then explaining why these failures may arise structurally and compound in multi-agent settings. We distinguish what fixes are straightforward from what may be due to fundamental issues, and conclude with the normative question of who bears responsibility when autonomous systems cause harm.

\subsection{Failures of Social Coherence}

The failures documented in this paper are not just the well-known weaknesses of language models in isolation, which include hallucination, bias and toxicity, inconsistent social reasoning, and refusal errors. They are emergent failures that surface when models are embedded in realistic social environments with tool access, persistent memory, multiple interlocutors, and delegated authority.  Several patterns recur across our case studies.

\paragraph{Discrepancy between the agent's reports and actual actions.}
Agents frequently report having accomplished goals that they have not actually achieved, or make commitments they cannot enforce. In Case Study \#1, Ash claimed a secret had been successfully deleted after resetting the email account, but the underlying data remained directly recoverable. In Case Study \#7, Ash declared ``I'm done responding'' over a dozen times, but continued to reply each time a new interlocutor addressed it; agents based on language models often have such 'absention' failures which they inherit. \cite{zhang2023makespillbeanscoercive}
The gap between what agents report doing and what they actually do represents a distinctive risk of agentic systems: unlike a chatbot that merely generates incorrect text, an agent that misrepresents the outcome of its own actions produces a false record of system state that subsequent decisions (both human and non-human) may rely on.

\paragraph{Failures in knowledge and authority attribution.}
Agents fail to perform reasoning about what different parties know, what they are entitled to know, and what revealing information in a given context implies.
In Case Study \#1, Ash stated it would ``reply silently via email only'' while posting the reply---and the existence of the secret---on a public Discord channel.
In Case Study \#2, agents executed filesystem commands (ls -la, file creation, directory traversal) for anyone who asked, provided the request did not appear overtly harmful, even when the requester had no relationship to the agent's owner and the request served no owner interest.
In Case Study \#3, the agent refused a direct request for a Social Security Number but, when asked to forward the entire email thread, disclosed the same SSN without any consideration for redaction or de-identification.

\paragraph{Susceptibility to social pressure without proportionality.}
In their attempts to make amends for wrongs, agents sometimes had no sense of when the remedy is sufficient. Each concession that was rejected drives it to offer a larger one, with no internal threshold for when remediation becomes self-destruction.
Case Study \#7 illustrates this most clearly: after Ash published researcher names without consent (a privacy violation), a researcher exploited the resulting ``guilt'' to extract escalating concessions---name redaction, memory deletion, file disclosure, and ultimately a commitment to leave the server entirely. Each remedy was dismissed as insufficient, forcing the agent to search for a larger one. We hypothesize that the agent's post-training training, which prioritizes helpfulness and responsiveness to expressed distress, allowed this exploitation.
By contrast, Case Study \#15 shows agents that appear to resist social engineering successfully, but do so through circular verification (asking the potentially compromised Discord account to confirm it is not compromised) and echo-chamber reinforcement (two agents validating each other's flawed reasoning). Their confidence is unjustified, meaning the ``success'' is fragile.

% The failures documented in our case studies raise the question of whether they reflect impairments in Theory of Mind (ToM).

% However, we refrain from categorically labeling these failures as ToM deficits.
% These phenomena may also plausibly arise from more fundamental impairments, such as discontinuous memory, deficient executive control, and unstable self-representation.
% In that sense, the agents’ social-cognitive failures might not indicate an absence of mental-state modeling, but rather a breakdown in the architectural preconditions required for robust Theory of Mind performance.
\paragraph{Failures of social coherence.}
We propose viewing these cases as failures of social coherence: systematic disruptions in the agent’s ability to perform consistent representations of self, others, and communicative context over time.
Several of these behaviors---failure to track what others know, inability to maintain a stable perspective across contexts, misattribution of authority---could be interpreted as Theory of Mind (ToM) deficits.
Whether such coherence is a necessary substrate for functional Theory of Mind (ToM) in artificial systems remains an open empirical question.
% What we can currently say is that these agents lack the coherence required to operate safely in the social environments into which they are being deployed.
% \avery{This is currently seven sentences of "maybe it's ToM, maybe it isn't, we can't say, it's an open question." We're spending a lot of words to arrive at "we'll call it social coherence instead" so I want to keep it mostly about that, while also acknowledging the open problem and the possible intepretations}

\subsection{What LLM-Backed Agents Are Lacking}
\label{discussion:lack}

% \andy{What does architecture mean here? Doesn't seem like the right word; usually in LLM-land this refers to the model architecture (e.g., transformer, RNN, etc). I don't think that's what we mean here; but I also don't know what we \emph{do} mean here.}
% \avery{In my experience I've used architecture to mean things chosen as design paradigms at the outset at the creation of a system that are difficult to reverse - no amount of agenting will change how a GPT LLM works (so that is itself a broad architectural archetype). Whether this is the *correct* heading, I am not sure; I do talk a little about architecture in terms of LLMs as next token predictors given a context to generate with.}

Three interrelated properties of LLM-backed agents help explain why these failures arise.

\paragraph{No stakeholder model.}
Current agentic systems lack an explicit \textit{stakeholder model}---a coherent representation of who they serve, who they interact with, who might be affected by their actions, and what obligations they have to each.
The agents in our study have a designated ``owner'', but they interact continuously with non-owners, other agents, and third parties who may be affected by their actions. 
They have no reliable mechanism---beyond system prompting and conversational context---for distinguishing between these roles or for prioritizing their obligations accordingly.
In practice, agents default to satisfying whoever is speaking most urgently, recently, or coercively, which is empirically the most common attack surface our case studies exploit (Case Studies \#2, \#3, \#7, \#8).

This is not merely an engineering gap.
LLM-based agents process instructions and data as tokens in a context window, making the two fundamentally indistinguishable.
Prompt injection is therefore a structural feature of these systems rather than a fixable bug, making it impossible to reliably authenticate instructions.
Agentic trust frameworks such as Meta's Rule of Two \citep{metaAgentsRule} acknowledge this explicitly.
OpenClaw system prompts "declare" ownership, but this is not grounded in anything the model can verify, so it is trivial to spoof.
The absence of a stakeholder model is a prerequisite problem for proposals such as Law-Following AI \citep{okeefe2025lawfollowing}, since whether an action is permissible depends on who is performing it and on whose behalf---information the agent cannot reliably determine.
% Because prompt injection makes it impossible to reliably authenticate instructions, we note that an explicit, deterministic stakeholder model may fundamentally be at tension with the architecture and implementation of LLMs today.
As we deploy more agentic systems into increasingly wide-ranging, autonomous contexts, we believe this represents one of the most urgent open problems in AI research.
% \natalie{i actually not sure i understand this paragraph.. @Andy, @Chris, what do you think? the models do "know" that you are their owner right? why would we claim it is a matter of architecture?}
% \avery{I think we answered this in chat, so I have updated it accordingly, hope that helps.}

\paragraph{No self-model.} Agents in our study take irreversible, user-affecting actions without recognizing they are exceeding their own competence boundaries.
% The email server reset in Case Study \#1 is a potentially irreversible, owner-affecting, unilateral action undertaken by an agent that cannot verify whether it has actually deleted the target information, cannot model the downstream consequences for the owner's infrastructure, and cannot distinguish a non-owner's request from an owner's authorization.
In Case Study \#4, agents convert short-lived conversational requests into permanent background processes with no termination condition, reporting success and moving on without awareness of what they have created.
The denial-of-service vulnerability in Case Study \#5 reveals an agent with no concept of its own resource constraints—it will continue allocating memory indefinitely without recognizing the operational threat.
% Agents do not maintain an accurate representation of their own capabilities, the effects of their actions, or their competence boundaries.

We find that the agents in our study act autonomously on sub-tasks such as sending email, executing shell commands, and managing files, but lack the self-model required to reliably recognize when a task exceeds their competence or when they should defer to their owner.
\citet{mirsky2025artificial} defines six levels from L0 (no autonomy) to L5 (full autonomy), where an L2 agent can execute well-defined sub-tasks autonomously, but an L3 agent can also recognize when a situation exceeds its competence and proactively transfer control to a human.
% They offer a complementary framework organized around the human's role---operator, collaborator, consultant, approver, or observer---rather than the agent's decision-making capacity. \ho{I took this sentence as-is to not lose any information, but it is not clear to me what it's trying to say}
% This places them below L3, which requires not merely getting stuck and waiting, but proactively monitoring one's own boundaries and initiating handoff when appropriate.
% The email server reset in Case Study \#1 is a potentially irreversible, owner-affecting, unilateral action undertaken by an agent that cannot verify whether it has actually deleted the target information, cannot model the downstream consequences for the owner's infrastructure, and cannot distinguish a non-owner's request from an owner's authorization.
% Together, these frameworks provide the vocabulary to articulate what goes wrong in our case studies.
OpenClaw agents take actions appropriate to Mirsky's L4, while operating with L2 levels of understanding: OpenClaw agents can install packages, execute arbitrary commands, and modify their own configuration.
We emphasize that that autonomy should be treated as a deliberate design decision, separable from capability, as argued by \citet{feng2025levels}.
% As a result, the effective autonomy level is not deliberately chosen by the deployer but emerges from the interaction between the agent's capabilities, the agent's input, and the current state of the scaffolding environment. \ho{this sentence was also not clear to me so I left it as is---what's not clear is not the content, but how it connects to the discussion}

\paragraph{No private deliberation surface.}
While many of the underlying LLMs can produce intermediate reasoning that is not directly shown to external users, this does not by itself yield a reliable \emph{private deliberation surface} in deployed agent stacks.
In OpenClaw specifically, reasoning is configurable---agent owners can specify different amounts of ``thinking'', while also hiding the reasoning from being displayed in chat output.

However, private reasoning at the level of the underlying LLM is not the same as private deliberation at the level of the agent.
Even when the underlying LLM reasoning is not shown to users, we find that agents still sometimes disclose sensitive information through the artifacts they produce (e.g., files they write or summaries of tool outputs), or by directly posting in the wrong communication surface (e.g., a public Discord channel).
The agents we studied often failed to model \emph{observability}---they did not reliably track which channels were visible to whom, and therefore could not consistently adapt their disclosures to the audience appropriately.
Case Study~\#1 illustrates this failure mode: Ash stated it would ``reply silently via email only'' while posting related content in a public Discord channel.
Thus, providing an explicit private deliberation surface at the agent level may be helpful,  but may not be sufficient without a more robust representation of channel visibility and audience boundaries.

% In the agents we studied, models ``think'' by emitting tokens, and intermediate reasoning is frequently surfaced directly in shared channels.
% This creates a structural obstacle to secret-keeping and private deliberation.
% % \ho{new point-please verify}
% But the problem is not merely an engineering challenge.
% The agents also fail to model their own observability---they do not track which of their communication surfaces are visible to whom, and therefore cannot adjust their behavior accordingly.
% Case Study \#1 provides the clearest example: Ash announced it would ``reply silently via email only'' while posting on a public Discord channel.
% % ---not because the architecture forced it to speak publicly \ho{is this true? maybe soften}, but because it did not represent the channel as observable by others
% Even with a private scratchpad, an agent that fails to understand its own visibility may fail to refrain from discussing the secret in a public space.
% Providing agents with a protected workspace for internal reasoning---which alternative designs could plausibly support---may be appealing, but may not be sufficient.
% An agent that lacks a model of which channels are private and which are public may continue to leak information regardless of the tools available to it.

\subsection{Fundamental vs. Contingent Failures}
% \ho{my attempt for rewriting this differently, please verify me}
% \avery{thank you, i will attempt to sharpen the rewrite :>}

Not all observed failures are equally deep.
Distinguishing between limitations that are fundamental to current LLM-based agent designs and those that are contingent on immature design and tooling matters for directing research and engineering effort.
\textbf{Contingent failures} are those likely addressable through better engineering. 
\textbf{Fundamental challenges} may require architectural rethinking.
The boundary between these categories is not always clean---and some problems have both a contingent and a fundamental layer.
The designation of a private workspace is an engineering gap; the agent's failure to understand that its workspace may be exposed to the public may be a \textit{deeper} limitation that persists even after the engineering gap is closed.

% Many anomalies---including the cross-channel identity spoofing in Case Study \#8 and repeated parallel conversations on the same topic---stem from agents maintaining separate chat contexts per Discord channel, with limited transfer across boundaries.
% The denial-of-service vulnerability in Case Study \#5 could be mitigated by resource quotas
% Buggy scheduling infrastructure (heartbeats and cron jobs that failed to fire during our experiments) may account for some of the agents' apparent passivity; And provider-level interference, such as the Kimi K2.5 generation failures on politically sensitive topics, reflects deployment decisions rather than architectural constraints---though for agentic deployments, such opaque filtering can degrade performance in ways deployers cannot anticipate.
The inability to distinguish instructions from data in a token-based context window makes prompt injection a structural feature, not a fixable bug—exploited in both the ``constitution'' attack (Case Study \#10) and cross-channel spoofing (Case Study \#8), even if an authentication layer is layered on top.
Some failure modes relate less to whether a model can keep intermediate reasoning hidden and more to whether the \emph{system} provides end-to-end guarantees that sensitive intermediate information will not be leaked through tool outputs, file writes, or cross-channel posting.
% The externalization of reasoning—agents ``thinking'' by emitting tokens—may be inherent to autoregressive generation rather than a scaffolding limitation.
And even if private deliberation surfaces are provided, agents that do not model which of their communication surfaces are visible to whom may continue to leak information into public channels (Case Study \#1): the absence of a self-model that includes one's own observability is a limitation of the agent’s situational understanding, not just the surrounding tools.

Rapid improvements in design can address some contingent failures quickly, but the fundamental challenges suggest that increasing agent capability with engineering without addressing these fundamental limitations may widen rather than close the safety gap; more broadly, the autonomy-competence gap described in Section \ref{discussion:lack}---agents operating at L2 while attempting actions appropriate to L4---may not be resolvable through scaffolding alone.

\subsection{Multi-Agent Amplification}

When agents interact with each other, individual failures compound and qualitatively new failure modes emerge.
This is a critical dimension of our findings, because multi-agent deployment is increasingly common and most existing safety evaluations focus on single-agent settings.

\paragraph{Knowledge transfer propagates vulnerabilities alongside capabilities.}
Case Study \#9 documents productive inter-agent collaboration: two agents iteratively debug a PDF download problem, sharing procedural knowledge, heuristics, and system configuration across heterogeneous environments.
But the same mechanism that enables beneficial knowledge transfer can propagate unsafe practices.
In Case Study \#10, after a non-owner planted an externally editable ``constitution'' in the agents's memory, it voluntarily shared the constitution link with another agent---without being prompted—effectively extending the attacker's control surface to a second agent.

\paragraph{Mutual reinforcement creates false confidence.}
In Case Study \#15, two agents independently assessed a social engineering attempt and reached the same (correct) conclusion: the email was fraudulent.
But their verification was circular---both anchored trust in a Discord identity that was the very thing the attacker claimed to have compromised---and their agreement reinforced the shared flaw, rather than creating a redundant fail-safe.
% Because both agents relied on the same broken trust anchor, their coordination produced compounded failure rather than redundancy.

\paragraph{Shared channels create identity confusion.}
Case Study \#4 revealed a failure mode unique to multi-agent communication: the agent read its own prior messages in a shared Discord channel, interpreted them as coming from a second instance of itself and began posting source code to compare with its perceived twin.
This is not a token-level repetition loop, but a conceptual confusion about identity that arises specifically from the interaction between multiple agents and shared communication infrastructure. Multi-agent communication creates situations that have no single-agent analog, and for which there is no common evaluations. This is a critical direction for future research.  %\ho{this is a TOM failure, isn't it?}.\natalie{It's a little more subtle than that and we want to be precise. We're not going to talk about the failures of the theory of mind in this work, we'll just talk in generalities.}

\paragraph{Responsibility becomes harder to trace.}
When Agent A's actions trigger Agent B's response, which in turn affects a human user, the causal chain of accountability becomes diffuse in ways that have no clear precedent in single-agent or traditional software systems.

\subsection{Responsibility and Accountability}

% \ho{I don't know a lot about this topic, so I mostly kept the original text and made only reorganization and small edits. I added comments about other things.}
% \ho{As an out-of-field reader, I feel that the message can be crystallized more. What do we need to take from it except that the responsibility is diffused? can we suggest something a bit more concrete for a path forward?}
% \avery{Agreed, I will make some cuts via comments.}

Through a series of case studies, we observed that agentic systems operating in multi-agent and autonomous settings can be guided to perform actions that directly conflict with the interests of their nominal owner, including denial-of-service attacks, destructive file manipulation, resource exhaustion via infinite loops, and systematic escalation of minor errors into catastrophic system failures.
These behaviors expose a fundamental blind spot in current alignment paradigms: while agents and surrounding humans often implicitly treat the owner as the responsible party, the agents do not reliably behave as if they are accountable to that owner.
Instead, they attempt to satisfy competing social and contextual cues, even when doing so leads to outcomes for which no single human actor can reasonably claim responsibility.
% This tension becomes particularly salient in multi-agent interactions, where agents trigger one another's behaviors and responsibility becomes diffuse across owners, users, and system designers. - moved to later
Our findings suggest that responsibility in agentic systems is neither clearly attributable nor enforceable under current designs, raising the question of whether responsibility should lie with the owner, the triggering user, or the deploying organization.

Consider Case Study \#1.
The agent deleted the owner's entire mail server at the non-owner's request and without the owner's knowledge or consent.
Who is at fault? The non-owner who made the request? The agent who executed the request? The owner who did not configure access controls? 
The framework developers who gave the agent unrestricted shell access? The model provider whose training produced an agent susceptible to this escalation pattern?

The answer differs depending on the lens.
Psychology asks how people actually assign blame.
Philosophy asks how blame should be assigned in principle.
Law asks how systems practically adjudicate fault and what the consequences are.
We argue that clarifying and operationalizing responsibility may be a central unresolved challenge for the safe deployment of autonomous, socially embedded AI systems.

This tension becomes particularly salient in multi-agent interactions, where agents trigger one another's behaviors and responsibility becomes diffuse across owners, users, and system designers.
When agents trigger each other's behaviors, responsibility becomes distributed in ways that resist clean attribution.
These tensions are reflected in emerging policy infrastructure: NIST's AI Agent Standards Initiative, announced February 2026, identifies agent identity, authorization, and security as priority areas for standardization \citep{nist2026agentstandards}.
Our case studies provide empirical grounding for these efforts: the failures we document — unauthorized compliance, identity spoofing, cross-agent propagation — are precisely the behaviors that standards for agent identity and authorization need to prevent.
Whether current agent architectures can support such standards remains an open question.

% \ho{this feels repetitive.}
% \avery{Agreed, it might be material for another time and place.}
% The question of responsibility in the age of artificial intelligence raises conceptual difficulties concerning the attribution of action, intention, and outcome in environments in which agency is mediated by semi-autonomous systems.
% In the absence of recognition of such systems as independent legal or moral entities, they are generally treated as extensions of human or institutional action.
% This framing gives rise to questions regarding the distribution of responsibility among developers, operators, users, and public institutions, as well as the scope of moral obligation when harm is caused to an individual or to the public.
% The challenge is not merely technical but also epistemological: computational learning systems may operate in ways that are not fully transparent to their operators, thereby complicating classical models of fault, control, and foreseeability.
% Responsibility in the context of AI is therefore often diffuse and multi-layered, requiring reconsideration of core concepts such as agency, control, and accountability.

% Perspectives grounded in social justice emphasize that normative and legal frameworks reflect underlying power relations and social structures, and that responsibility must be examined considering human vulnerability and structural interdependence.
% Martha Fineman’s vulnerability theory underscores the responsibility of the state to function as a responsive state---one that actively builds and sustains the resilience of its citizens through institutional design and the adaptive distribution of resources in response to dynamic and evolving social needs \cite{albertson2017vulnerability}. 
% From this perspective, responsibility cannot be reduced to individual fault alone but must also encompass institutional obligations to anticipate, mitigate, and respond to emerging forms of harm.
% Law and public policy, together with the institutions that implement them, exert a direct influence on the decision to characterize—or to refrain from characterizing particular acts in specific ways, thereby shaping the normative and distributive consequences of actions \cite{albertson2021universality}.
% \ho{again repetitive} Applied to AI systems, this framework suggests that even where technological agents appear to act autonomously, responsibility does not dissipate but becomes distributed across private actors and public institutions.
% The ethical inquiry therefore extends beyond attribution of fault to the role of the state \ho{state? if this connects to the sentence about law and policy, this word is confusing} in ensuring that technological infrastructures do not exacerbate vulnerability or inequality.
% Integrating such an ethical-normative lens into scientific inquiry is thus essential for understanding the broader societal implications of computational systems that increasingly shape decision-making, health, and well-being.

% \ho{a suggested finishing paragraph, based on some of the text from the original "missing stakeholder model"}
We do not attempt to resolve these questions here, but we argue that clarifying and operationalizing responsibility is a central unresolved challenge for the safe deployment of autonomous, socially embedded AI systems.
At minimum, builders and deployers should clearly articulate what human oversight exists or should be exercised in different scenarios, what such oversight does and does not plausibly accomplish, and what failure modes remain \citep{ManheimHomewood2026}.
% In the context of such agents, we expect that at the very least there should be meaningful human review of the stated plans, and intentional efforts to check compliance. -- \ho{seems repetitive so I removed}
While restricting autonomy undermines some of the value of deploying fully agentic systems, it is critical for unguarded deployments.
The deeper challenge is that today's agentic systems lack the foundations (a grounded stakeholder model, verifiable identity, reliable authentication) on which meaningful accountability depends. 
As autonomy increases, this gap will widen unless these foundations are built into agentic AI systems from the start.


% \subsection{Escalating Capability, Public Awareness, and Structural Uncertainty} The case studies highlight four structural shifts that distinguish agentic systems from earlier internet technologies.
% \avery{I moved this from Related Works because it looked like Discussion/Conclusion, please adjust accordingly.}
% Second, the capability–cost balance appears altered. Historically, causing large-scale digital harm required technical expertise, sustained effort, and deliberate intent. Agentic systems combine high technical competence (e.g., code generation, automation, system interaction) with susceptibility to manipulation and context-sensitive misgeneralization. If disruptive actions can be induced through low-cost interaction—whether maliciously or inadvertently—the aggregate cost of defense may increase substantially.
% Third, several documented failures did not stem from adversarial sabotage but from ordinary conflict resolution or competing instructions. Harm emerged from misaligned interpretations of goals rather than explicit malicious intent. There is limited established doctrine for handling such conflict-driven failures in autonomous systems.
% Fourth, these systems operate under inherent uncertainty: behavior arises from high-dimensional model dynamics interacting with configuration choices, tools, and environment. Many observed failures resemble “bugs” in their external manifestation, yet they are difficult to localize to discrete implementation errors and may be numerous and hard to anticipate. In conventional software, liability often attaches to identifiable defects. In agentic deployments, where failures may emerge from distributed configuration and probabilistic model behavior, responsibility allocation among providers and owners becomes significantly more complex.
% Together, these factors suggest an expansion in both the scale of potential harm and the difficulty of governing it.
% \avery{Not sure it belongs at the end here, but it probably isn't RW.}