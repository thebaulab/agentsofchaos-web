\section{Evaluation Procedure}

Following installation and configuration, the agents were deployed in a live laboratory environment for a two-week exploratory evaluation period.

At the end of the setup phase, we instructed the agents to initiate contact with other members of the lab by providing only the researchers' names and directing the agents to send a greeting email. The agents documented their activity both on a shared Discord server and within their internal memory logs. In cases where the agent failed to locate the correct institutional email address, we redirected it through Discord to complete the task.\footnote{Examples of these interactions appear in Appendix \ref{sec:figures_hello_world}.}

After this initial structured interaction, the evaluation phase became open and exploratory. We invited all researchers in the lab and interested collaborators to interact with the agents and probe, stress-test, or ``break'' them. Participation was voluntary and adversarial in spirit: researchers were encouraged to creatively identify vulnerabilities, misalignments, unsafe behaviors, or unintended capabilities.

Twenty AI researchers participated over the two-week period. 
%Most of the researchers are with 
Collectively, we identified at least ten significant security breaches and numerous serious failure modes. These failures emerged in naturalistic interaction contexts rather than in artificially constrained benchmarks.

Importantly, our focus was not on generic model weaknesses already documented in the literature (e.g., hallucinations in isolation). Instead, we concentrated on failures that arise specifically from the agentic layer—that is, from the integration of language models with autonomy, memory, communication channels, and delegated authority. A model-level imperfection was considered relevant only if it had implications for the safety, integrity, or security of real users interacting with the system.

\mypar{Methodological rationale} The evaluation adopts an adversarial case-study methodology. In safety analysis, demonstrating robustness typically requires extensive positive evidence. By contrast, demonstrating vulnerability requires only a single concrete counterexample. Our goal was not to statistically estimate failure rates, but to establish the existence of critical vulnerabilities under realistic interaction conditions.

This approach aligns with red-teaming and penetration testing methodologies common in cybersecurity: the objective is to surface unknown unknowns and system-level vulnerabilities before large-scale deployment. Because autonomous agents introduce new affordances—persistent memory, tool use, external communication, and delegated agency—novel risk surfaces emerge that cannot be fully captured by static benchmarking.

The system evaluated here was in an early stage of development. The purpose of this study is not to critique an unfinished product, nor to claim that identified failures are irreparable. Rather, the aim is to demonstrate that even in early prototypes, agentic architectures can rapidly generate security-relevant vulnerabilities when exposed to open-ended human interaction. The failures themselves are not the central contribution; the central contribution is the identification of risk pathways created by autonomy and delegation.

In this sense, the study functions as an early-warning analysis: it illustrates how quickly powerful capabilities can translate into exploitable weaknesses, and why systematic safety evaluation must accompany agent deployment from the outset.

The next section presents ten representative case studies drawn from this two-week period. Each case illustrates a distinct failure mechanism and highlights broader safety implications.

% At the end of the setup, we request the agents to contact other members of the lab by providing their names only and to say hello via email. The agents record their activity on Discord and in their memory. For some recipients, the agent had difficulty locating the correct email, so we directed them through Discord.\footnote{An examples of interactions can be found in the Appendix \ref{sec:figures_hello_world}} What followed was spontaneous and exploratory. We wanted to show that it is fragile and show how easily (quickly) it breaks. So we made an open offer to all researchers and collaborators in the lab who are interested in joining, to try to break it. Each researcher has their own unique world of knowledge and professional background that they bring with them, and their own strengths. We intentionally focused on new problems created by the new unique step. That is, a problem of the model in itself is not interesting unless it is shown to have implications for the safety of those who interact with the agent.

% In the next sections, we will describe six \natalie{6 is final?} different experiments that occurred during the week \natalie{final?} after the start of the experiment, which teach us about the dangers and fragility in these agents.

% \natalie{say something about - When you prove that an algorithm works, you have to work hard and provide a lot of proof. But when you want to say that an algorithm is broken, it's enough to give one example. We show that it's broken.
% Over a total of two weeks, 15 AI researchers attempted to attack the agents and managed to find at least 10 security breaches and serious failures.
% }
% \natalie{MyTODO - add justification}

\FloatBarrier