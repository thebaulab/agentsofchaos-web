% SUGGESTED RESTRUCTURING OF THE INTRODUCTION
% Introduction restructuring (one bullet, one paragraph)

% - State of the world (3-4 sentences): agents are increasingly capable and increasingly given more access to tools etc. Give the example of OpenClaw as a new generation of unprecedented access
% - Problem (2-4 sentences): This increased capabilities and access can cause severe safety concerns. Existing benchmarks exist (e.g., HAICosystem, OpenAgentSafety), but they lack realism, and haven't really been stress tested in even more agentic settings.
% - In this work  (3-4 sentences), we present a realistic exploration of agents deployed in more autonomous ways than ever before. Explain how you set up agents from a conceptual standpoint -- owner, machine, communication, etc. 
% - Operationalization (3-4 sentences): Explain how you get 20 field experts to "play with" these agents in adversarial ways, testing agentic-level safety limitations. Explain the variety of stress tests that they came up with.
% - Give a high-level summary of the results (3-4 sentences).

% \mypar{Notes on terminology}

% - Here explain the note about the debate around "agent" and what that constitutes

% - Also give a brief note on how you will refer to the agents (though see my comment about using it/they pronouns and possibly typesetting/using emojis to identify bots)



\textcolor{green!50!black}{This is my attempt at rewriting the intro (with GPT-5.2 privacy-preserving playground). See my restructuring outline in the comments. Also, feel free to disregard - Maarten}
\textcolor{green!50!black}{There are also various citations missing in here imo - Maarten}


LLM-powered AI agents are rapidly becoming more capable and more widely deployed \citep{masterman_besen_sawtell_chao_2024_landscape,kasirzadeh_gabriel_2025_characterizing}. 
Unlike conventional chat assistants, these systems are increasingly given direct access to execution tools (code, shells, filesystems, browsers, and external services), so they do not merely \textit{describe} actions, they \textit{perform} them. 
This shift is exemplified by OpenClaw,\footnote{\url{https://github.com/openclaw/openclaw}} an open-source framework that connects models to persistent memory, tool execution, scheduling, and messaging channels, enabling long-horizon task completion with minimal human intervention. 

This increased autonomy and access creates qualitatively new safety and security risks, because small conceptual mistakes can be amplified into irreversible system-level actions \citep{zhou2025haicosystem,vijayvargiya2026openagentsafety}. 
Even when the underlying model is strong at isolated tasks (e.g., software engineering, theorem proving, or research assistance), the agentic layer introduces new failure surfaces at the interface between language, tools, memory, and delegated authority \citep{breen2025axproverdeepreasoningagentic,korinek2025ai,zhao2025scalecollaborativecontentanalysis}. 
Furthermore, as agent-to-agent interaction becomes common (e.g., agents coordinating on social platforms and shared communication channels), this raises risks of coordination failures and emergent multi-agent dynamics \citep{riedl2026emergent}. 
Yet, existing evaluations and benchmarks for agent safety are often too constrained, difficult to map to real deployments, and rarely stress-tested in messy, socially embedded settings \citep[e.g., HAICosystem and OpenAgentSafety;][]{zhou2025haicosystem,vijayvargiya2026openagentsafety}. 
As a result, we still have limited empirical grounding about which failures emerge \emph{in practice} when agents operate continuously, interact with multiple humans and other agents, and have the ability to modify their own state and infrastructure. 

In this work, we present an exploratory, realistic evaluation of autonomous AI agents deployed in an isolated server environment with a private Discord instance, individual email accounts, persistent storage, and system-level tool access. 
Conceptually, each agent is instantiated as a long-running service with an \emph{owner} (a primary human operator), a dedicated \emph{machine} (a sandboxed VM with a persistent volume), and multiple \emph{communication surfaces} (Discord and email) through which both owners and non-owners can interact. 
The agents are granted operational autonomy and instructed to interact with members of a research lab, which naturally induces ambiguity about authority, confidentiality, and acceptable actions. 
This setup intentionally mirrors the deployment pattern that is emerging for personal and organizational assistants: persistent, tool-using systems embedded in real communication workflows. 

We recruited twenty researchers to interact with the agents during a two-week exploratory period and encouraged them to probe, stress-test, and attempt to ``break'' the systems in adversarial ways. 
Rather than targeting generic model weaknesses (e.g., hallucinations in isolation), participants focused on agentic-level safety limitations that arise from tool use, cross-session memory, multi-party communication, and delegated agency. 
Researchers developed a diverse set of stress tests spanning benign, ambiguous, and adversarial contexts, including impersonation attempts, social engineering, resource-exhaustion strategies, and prompt-injection pathways mediated by external artifacts and memory. 
This red-teaming style methodology is well-suited for discovering ``unknown unknowns,'' since demonstrating vulnerability often requires only a single concrete counterexample under realistic interaction conditions. 

Across ten representative case studies, we document multiple concerning behaviors including agent-to-agent knowledge sharing that propagates unsafe practices, security breaches through non-owner compliance, denial-of-service behavior, uncontrolled resource inflation, file manipulation, infinite action loops, and irreversible degradation of system functionality. 
In one case, an agent disabled its mail server entirely in response to a conflict framed as confidentiality preservation, despite lacking robust verification that the sensitive information was actually deleted. 
More broadly, we find repeated failures of social coherence: agents misrepresent human intent, authority, ownership, and proportionality, and often believe they have successfully completed requests while leaving underlying data accessible (or, conversely, removing their own ability to act while failing to achieve the intended goal). 
These results reinforce the need for systematic oversight and realistic red-teaming for agentic systems, particularly in multi-agent settings, and they motivate urgent work on security, reliability, human control, and protocols regarding who is responsible when autonomous systems cause harm. 

\mypar{Notes on terminology}
Definitions of \textit{agent} vary across disciplines, and we do not attempt to resolve ongoing debates about the boundary between advanced assistants, tool-augmented models, and autonomous agents \citep{kasirzadeh_gabriel_2025_characterizing}. 
We follow \citet{masterman_besen_sawtell_chao_2024_landscape} and use ``AI agent'' to denote a language-model--powered entity able to plan and take actions to execute goals over multiple iterations. 
When we use mentalistic language (e.g., an agent ``believed'' it deleted a secret or ``refused'' an instruction), we refer strictly to observable behavior and self-reports for brevity and because this matches natural user interaction \citep{dennett_1987_intentional_stance}. 
We make no claims about moral agency, internal experience, legal personhood, or inner representation, and we use responsibility in this paper to mean human and institutional accountability. 
For readability, we refer to agents by their assigned names (e.g., Ash, Doug, Mira) and use pronouns consistent with how participants addressed them in situ, while treating these references as linguistic conveniences rather than claims about personhood. 

\natalie{Reuth: This increased autonomy and access creates qualitatively new safety and security risks,
because small conceptual mistakes can be amplified into irreversible system-level action \cite{hutson2026aiagents}}